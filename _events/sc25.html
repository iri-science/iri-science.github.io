---
layout: page
title: SC25
subtitle: Discover how DOE's IRI Program is shaping the future of supercomputing and scientific discovery
headline: IRI @ SC25
image: images/sc25-banner.png
date: 2025-11-8
category: Events
---

<section class="section-sm">
    <div class="container text-center mb-5">
        <img src="/images/sc25-logo.png" style="width: 20em;" />
    </div>
    <div class="container mb-5">
        <p>There will be multiple opportunities to interact with IRI committee members, learn about the IRI program, and
            see demonstrations of IRI capabilities at the <a href="https://sc25.supercomputing.org/"
                target="_blank">SC25 conference</a>.</p>
    </div>

    <div class="container mb-5">
        <h3 class="mb-3">IRI Activities at SC25</h3>

        <h4>Tuesday, November 18, 2025</h4>
        <div class="content mb-5">
            <table>
                <thead>
                    <tr>
                        <th style="min-width: 6em;">Time</th>
                        <th style="min-width: 10em;">Location</th>
                        <th>Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>12:15pm</td>
                        <td>Room 130</td>
                        <td>
                            <span class="event-label">BoF</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=bof253&sess=sess419" target="_blank">
                                <strong>IRI Interfaces at Work: Prototypes, Progress, and Community Feedback</strong>
                            </a><br />
                            DOE has recently launched the Integrated Research Infrastructure (IRI) program, which is designed to enable new modes of integrated science across DOE user facilities. Common or unified interfaces are needed for these workflows to seamlessly orchestrate resources across high-performance computing, data, and network providers. The IRI interfaces working group composed of members of ASCR user facilities has spent the last year designing and implementing APIs for compute facilities. Here we’d like to present our current development status, deployed prototypes, and future roadmap for discussion. We are seeking feedback from the user community to help guide these IRI efforts.</td>
                    </tr>
                    <tr>
                        <td>3:00pm</td>
                        <td>DOE Booth 3802</td>
                        <td>
                            <span class="event-label">Demo</span>
                            <a href="https://scdoe.info/demonstrations/" target="_blank">
                                <strong>IRI Multifacility Light Sources Pathfinder</strong>
                            </a><br />
                            The Integrated Research Infrastructure Pathfinder Projects advance the IRI program by implementing multi-facility workflows in a way that provides actionable information for other projects and the program as a whole. This demonstration will showcase progress over the past year by showing the ability for multiple light-source beamlines to run at multiple HPC facilities. We will show this for beamlines at three light source facilities — the ALS, LCLS and APS — each able to run at multiple HPC facilities.</td>
                    </tr>
                </tbody>
            </table>
        </div>

    </div>

    <br /><br />
    <div class="container mb-5">
        <h3 class="mb-3">IRI-adjacent Activities at SC25</h3>
        <p>There will be several sessions, events, workshops, and demonstrations that illustrate aspects of the IRI
            ecosystem, including workflows, design patterns, and specific technologies. While these are not explicitly
            part of the IRI program, conference attendees who are interested in IRI might find these activities to be of
            particular interest.</p>

        <h4>Sunday, November 16, 2025</h4>
        <div class="content mb-5">
            <table>
                <thead>
                    <tr>
                        <th style="min-width: 6em;">Time</th>
                        <th style="min-width: 10em;">Location</th>
                        <th>Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>9:00am</td>
                        <td>Room 240</td>
                        <td>
                            <span class="event-label">Talk</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=ws_xloop105&sess=sess119" target="_blank">
                                <strong>Accelerating Advanced Light Source Science Through Multi-Facility HPC Workflows</strong> &ndash; XLOOP Workshop
                            </a><br />
                            Synchrotron light sources support a wide array of techniques to investigate materials, often producing complex, high-volume data that challenge traditional workflows. At the Advanced Light Source (ALS), we developed infrastructure to move microtomography data over ESnet to ALCF and NERSC, where CPU- and GPU-based algorithms generate 3D reconstructed volumes of experimental samples. We employ two data movement and reconstruction models: real-time processing as data streams directly to NERSC compute nodes, and automated file transfer to NERSC and ALCF file systems. The streaming pipeline provides users with feedback in under ten seconds, while the file-based workflow produces high-quality reconstructions suitable for deeper analysis in 20-30 minutes. This infrastructure allows users to leverage HPC resources without direct access to backend systems. We plan to extend this architecture to more endstations, supporting our beamline scientists and users.
                    </tr>
                    <tr>
                        <td>9:25am</td>
                        <td>Room 240</td>
                        <td>
                            <span class="event-label">Talk</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=ws_xloop106&sess=sess119" target="_blank">
                                <strong>Streaming X-ray Detector Data to Remote Facilities Using EJFAT</strong> &ndash; XLOOP Workshop
                            </a><br />
                            Propelled by the increasing need for the near real-time feedback for user experiments on its X-ray beamlines, the Advanced Photon Source continues to investigate the use of streaming workflows, with several of those being successfully deployed on its local computing infrastructure. With ever-growing data volumes and compute resource needs, the ability to analyze beamline data at remote facilities is becoming more and more important. In this paper we investigate the possibility of using ESnet JLab FPGA Accelerated Transport (EJFAT) project infrastructure to bring X-ray detector data directly from the instrument into an analysis application running at a remote high performance computing center. To that end, we describe successful integration of PvaPy, a Python API for the EPICS PV Access protocol, with the EJFAT software library. We also discuss potential use cases, as well as illustrate system performance in terms of maximum achievable frame and data rates in a test environment.
                    </tr>
                    <tr>
                        <td>9:50am</td>
                        <td>Room 240</td>
                        <td>
                            <span class="event-label">Talk</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=misc182&sess=sess119" target="_blank">
                                <strong>The LCLStream Ecosystem for Multi-Institutional Dataset Exploration</strong> &ndash; XLOOP Workshop
                            </a><br />
                    </tr>
                    <tr>
                        <td>10:30am</td>
                        <td>Room 266</td>
                        <td>
                            <span class="event-label">Panel</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=miscp130&sess=sess221" target="_blank">
                                <strong>Integrated Infrastructure Panel</strong> &ndash; INDIS Workshop
                            </a><br />
                            In this Integrated Research Infrastructure panel we will explore the network innovations needed for the emerging Artificial Intelligence, Super- and Quantum Computing facilities in the context of the mission infrastructures and American Science Cloud. Moreover we will explore several different testbed initiatives around the world that will support the research and innovation on network infrastructure.
                    </tr>
                </tbody>
            </table>
        </div>

        <h4>Tuesday, November 18, 2025</h4>
        <div class="content mb-5">
            <table>
                <thead>
                    <tr>
                        <th style="min-width: 6em;">Time</th>
                        <th style="min-width: 10em;">Location</th>
                        <th>Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>12:15pm</td>
                        <td>Room 123</td>
                        <td>
                            <span class="event-label">BoF</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=bof223&sess=sess415" target="_blank">
                                <strong>Trusted Research Environments for AI and Integrated Science</strong>
                            </a><br />
                            We explore the multifaceted aspects of managing risk for research projects involving sensitive data and AI models which depend deeply on supercomputing infrastructures. This risk spectrum spans traditional technical cyber controls as well as policy and sociological (including human factors) risks. In the context of multi-facility, multi-institutional workflows such as IRI and the American Science Cloud (AmSC), our goal is to advance progress in developing secure and trustworthy infrastructures for AI and integrated science. Three intertwined challenges emerge as we advance this vision for IRI and AmSC: technological, policy, and sociological. With the rise of AI and the increasing use of sensitive data for training models, our goal is to leverage this BoF to build a community of practice that will advance a secure and trusted research environment (TRE) that addresses challenges in all three domains. How do we best achieve a TRE that is transparent, reproducible, ethical, secure, worthwhile, and collaborative, with clear data provenance and assurance? How might trust be rightfully earned and retained through modern workflows through managed risk and secure governance? The outcomes from this BoF are: (1) explore TRE challenges in the age of AI and science integration; (2) identify alignment and divergence in TRE practices (3) learn from complementary efforts across institutions around the globe; (4) build a community of practice committed to trustworthy integrated science in the age of AI. We invite an audience with interests in these topics to participate in advancing these outcomes.
                    </tr>
                    <tr>
                        <td>2:00pm</td>
                        <td>DOE Booth 3802</td>
                        <td>
                            <span class="event-label">Demo</span>
                            <a href="https://scdoe.info/demonstrations/" target="_blank">
                                <strong>Prototyping an AmSC/IRI Agent for Multi-Facility Portable Computing Jobs</strong>
                            </a><br />
                            A key capability of the anticipated Integrated Research Infrastructure (IRI) is to run compute jobs across distributed facilities within the lifecycle of the same application workflow. Data storage, access and movement must be tightly integrated with job execution within this workflow. This demo will present an overview of an IRI agent that integrates with example resource providers, including HTCondor (compute job meta-scheduler), SENSE (cross-facility networking), Janus (data movement), EJFAT (data streaming and processing) and an example common interface for placing, moving and executing jobs over diverse facilities. Following the overview, the EJFAT team (ESnet/Jefferson Lab FPGA Accelerated Transport) will show examples of real time streaming of raw instrument data from Berkeley Lab’s Advanced Light Source and Argonne’s Advanced Photon Source (APS). This demo showcases a proof-of-concept IRI workflow where compute jobs are placed based on available resources across facilities, and datasets are moved and accessed via on-demand network and data transfer services. This demonstration will discuss how future IRI agents and interfaces will help orchestrate such workflows with general resource providers, including the American Science Cloud.
                    </tr>
                    <tr>
                        <td>3:00pm</td>
                        <td>DOE Booth 3802</td>
                        <td>
                            <span class="event-label">Demo</span>
                            <a href="https://scdoe.info/demonstrations/" target="_blank">
                                <strong>Next-Generation Data Infrastructure to Advance AI in Biotechnology</strong>
                            </a><br />
                            Advances in AI, machine learning and data generation are changing the data and computing landscape in biotechnology rapidly. The Data Ecosystem infrastructure being developed by the DOE SC Office of Biological and Environmental Research is enabling data exploration, modeling and inquiry at scale across the DOE HPC ecosystem via the JGI Analysis Workflow Service (JAWS) supporting Joint Genome Institute (JGI), Environmental Molecular Sciences Laboratory (EMSL), and the BER-ASCR OPAL AI and lab automation project. This demonstration will explore how the JGI distributes genomic analyses across DOE computing facilities on behalf of its users without their intervention, opening opportunities for larger-scale analysis and greater resilience. JAWS demonstrates how the DOE Integrated Research Infrastructure (IRI) vision can be realized in practice. JAWS is a multi-site workflow execution system that orchestrates large-scale, high-throughput genomic workflows across the ASCR computing facilities and the DOE HPC infrastructure, including NERSC (Perlmutter), LBNL (Dori and Lawrencium), EMSL (Tahoma), ALCF (Crux), and OLCF (Defiant), while ensuring secure, automated data movement via Globus and portable execution through containers. By integrating community workflow standards with DOE’s HPC ecosystems, JAWS provides a scalable, fault-tolerant backbone for diverse scientific campaigns, from metagenome analysis in the National Microbiome Data Collaborative (NMDC) to multi-omics integration for carbon cycling studies. Within the IRI Testbed, JAWS showcases how interoperable services can bridge experimental, observational and computational facilities, enabling resilient cross-facility workflows, accelerating feedback between instruments and simulations, and empowering researchers with FAIR, AI-ready data products. This demonstration will highlight both JAWS’ technical capabilities and the science it enables.
                    </tr>
                    <tr>
                        <td>5:00pm</td>
                        <td>DOE Booth 3802</td>
                        <td>
                            <span class="event-label">Demo</span>
                            <a href="https://scdoe.info/demonstrations/" target="_blank">
                                <strong>Terabit scale streaming from DOE Nuclear Physics Facilities to HPC</strong>
                            </a><br />
                            The Facility for Rare Isotope Beams (FRIB) is a DOE Office of Science user facility focused on studying problems of national interest in low-energy nuclear physics. Real-time or near real-time (nearline) analysis methods are critical tools for enabling FRIB science as new detectors and data acquisition technologies which allow for higher data rates and volumes are incorporated into laboratory systems. In addition, many experiments rely on computationally intensive analysis tasks where real-time processing on local computer systems is not feasible. EJFAT/E2SAR provides a convenient platform for streaming data from FRIB to offsite locations, for example, to NERSC, where more computational resources are available to experimenters. An automated workflow was developed to stream data from an FRIB experiment to NERSC, process digitized detector waveform traces at NERSC, extract features of interest from the trace data and send the results back to FRIB. This demonstration shows an example of how EJFAT/E2SAR can be used in future applications to connect the data stream from a live experiment to external high-performance computing resources.
                    </tr>
                    <tr>
                        <td>5:15pm</td>
                        <td>Room 274</td>
                        <td>
                            <span class="event-label">BoF</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=bof157&sess=sess437" target="_blank">
                                <strong>Workflows Community: Bridging Intelligent Workflows with Quantum and HPC for Scientific Discovery</strong>
                            </a><br />
                            This BoF will convene the workflows community to discuss emerging directions in scientific workflow execution, including agentic workflows, integration of high-performance and quantum computing workflows, and coordinated allocation and scheduling across experimental and computing facilities. A central focus will be on ensuring end-to-end resource availability when workflows depend on limited instrument time and distributed infrastructure. The session will also address the need for infrastructure and policy reforms to support intelligent, cross-facility execution. Through interactive discussions, participants will explore collaborative strategies to enable resilient, scalable, and adaptive workflows that meet the evolving demands of scientific discovery.
                    </tr>
                </tbody>
            </table>
        </div>

        <h4>Wednesday, November 19, 2025</h4>
        <div class="content mb-5">
            <table>
                <thead>
                    <tr>
                        <th style="min-width: 6em;">Time</th>
                        <th style="min-width: 10em;">Location</th>
                        <th>Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>3:00pm</td>
                        <td>DOE Booth 3802</td>
                        <td>
                            <span class="event-label">Demo</span>
                            <a href="https://scdoe.info/demonstrations/" target="_blank">
                                <strong>FNAL-NERSC Raw Data Streaming for IRI Demo</strong>
                            </a><br />
                            IRI demonstration: live-streaming of raw DAQ data from FNAL to NERSC for real-time analysis, with an expected flow at ~50Gbps, to explore how IRI can address the needs of triggerless or streaming DAQ architectures.
                    </tr>
                    <tr>
                        <td>5:15pm</td>
                        <td>Room 130</td>
                        <td>
                            <span class="event-label">BoF</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=bof246&sess=sess464" target="_blank">
                                <strong>Real-Time Scientific Data Streaming to HPC Nodes: Challenges and Innovations II</strong>
                            </a><br />
                            Emerging scientific needs are driving a new class of workflows that require near real-time processing, urgent computing, and time-sensitive decision-making. These workflows must bypass traditional buffering and shared file systems, instead relying on direct data streaming over WAN into HPC compute nodes. Latency and variability must be minimized to enable timely responses and experiment steering. This BoF will explore strategies, technologies, and policies to support these streaming workflows at scale, with a focus on building shared infrastructure and community practices to routinely enable this next generation of high-impact, time-critical scientific computing.
                    </tr>
                </tbody>
            </table>
        </div>

         <h4>Thursday, November 20, 2025</h4>
        <div class="content mb-5">
            <table>
                <thead>
                    <tr>
                        <th style="min-width: 6em;">Time</th>
                        <th style="min-width: 10em;">Location</th>
                        <th>Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>12:15pm</td>
                        <td>Room 125</td>
                        <td>
                            <span class="event-label">BoF</span>
                            <a href="https://sc25.conference-program.com/presentation/?id=bof242&sess=sess477" target="_blank">
                                <strong>Integrated HPC: Lessons Learned From Multi-Facility Integration Projects Around the World</strong>
                            </a><br />
                            Science and computing is increasingly integrated, as large-scale scientific and computing challenges take on a national, and even international, scale. The need for resources at multiple facilities may be driven by access to site-specific hardware, security policy or to ensure resilient operations. Deeper integration between facilities can create efficiencies for scientists, funding agencies and the facilities themselves, but also exposes site incompatibilities in both technology and culture.
In this BOF we bring together seasoned experts in integrating supercomputing resources across institutions to discuss the challenges and opportunities of creating and managing the frameworks (political and technical) needed to integrate HPC.
                    </tr>
                </tbody>
            </table>
        </div>

    </div>
</section>
